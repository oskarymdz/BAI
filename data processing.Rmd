---
title: "Intership project"
output: html_document
date: "2025-09-27"
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,        
  eval = TRUE,         
  cache = FALSE,
  warning = FALSE,     
  message = FALSE,    
  results = 'hide'     
)
options(width = 80, digits = 3)


library(dplyr)
library(tidyverse)
library(here)
library(galah)
library(sf)
library(spData)
library(ggplot2)
library(Cairo)
library(kableExtra)
library(bookdown)
library(tibble)
library(purrr)
library(knitr)
library(readr)
library(stringr)
library(purrr)
```





#### Data description

The ‘Cafes and Restaurants with Seating Capacity’ dataset is sourced from the City of Melbourne’s Census of Land Use and Employment (CLUE). It provides annual records of business establishments located within the Melbourne local government area from 2002 to 2023. The data can be retrieved via the [City of Melbourne Open Data Portal](https://data.melbourne.vic.gov.au/explore/dataset/cafes-and-restaurants-with-seating-capacity/information).


<div style="font-size: 80%;">
```{r var-types,message = FALSE, warning = FALSE, results = 'show'}
# Read the cleaned CSV files into R environment
df <- read_csv("data/cafes-and-restaurants-with-seating-capacity.csv")

# Get column names and types
col_types <- map_chr(df, ~ class(.x)[1])
# Split into two groups
first_half <- col_types[1:7]
second_half <- col_types[8:15]
# Build a 2-row tibble with padding between groups
df_split_summary <- tibble::tibble(
  !!!setNames(as.list(first_half), names(first_half)),
  !!!setNames(as.list(second_half), names(second_half))
)
# Use HTML-based styling but ensure it's compatible with PDF conversion
kbl(df_split_summary, format = "html", caption = "Variable Types in CLUE Dataset", 
    table.attr = 'id="tab:var-types"') %>% # Add explicit ID
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"), 
                full_width = FALSE, 
                position = "center")
```
</div>


The Table \@ref(tab:var-types) shows the column names and data types in the dataset. It includes numbers, text, and location data . This mix of data types makes it useful for different kinds of analysis. The variable names in the dataset are generally clear and descriptive.






<div style="font-size: 80%;">
```{r  summary, message = FALSE, warning = FALSE, results = 'show'}


# Dimensions
n_rows <- nrow(df)
n_cols <- ncol(df)

# Duplicates
n_duplicates <- sum(duplicated(df))

# Missing values
missing_summary <- colSums(is.na(df))
total_missing <- sum(missing_summary)
missing_cols <- names(missing_summary[missing_summary > 0])
missing_cols_str <- paste(missing_cols, collapse = ", ")

# Negative or zero seating values
invalid_seating <- sum(df$`Number of seats` < 0, na.rm = TRUE)

# Create horizontal summary table
df_summary <- tibble(
  `Number of Rows` = n_rows,
  `Number of Columns` = n_cols,
  `Number of Duplicate Rows` = n_duplicates,
  `Total Missing Values` = total_missing,
  `Columns with NA Values` = length(missing_cols),
  `Names of Columns with NA Values` = missing_cols_str,
  `Non-positive Seating Values` = invalid_seating
)

# Display the summary table with border and proper alignment
kable(df_summary, format = "html", caption = "Summary of Dataset Structure and Quality Checks") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"), full_width = F, position = "center")



```
</div>





As shown in Table \@ref(tab:summary), the dataset has 63,121 rows and 15 columns, with no duplicate entries and negative seating number. There are 1,581 missing values, all in the location-related columns (Longitude, Latitude, location). 


<div style="font-size: 80%;">
```{r year, message = FALSE, warning = FALSE, results = 'show'}

library(tibble)
library(knitr)

# Get sorted unique census years
census_years <- sort(unique(df$`Census year`))

# Create one-row, one-cell table
census_table <- tibble(`Census Years` = paste(census_years, collapse = ", "))


kable(census_table, format = "html", caption = "Unique Census Years in the Dataset") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"), full_width = F)

```
</div>

Table \@ref(tab:year) shows the dataset covers 22 unique census years, ranging from 2002 to 2023. 


<div style="font-size: 80%;">
```{r  type, message = FALSE, warning = FALSE, results = 'show'}
library(dplyr)
library(knitr)
library(kableExtra)

library(dplyr)
library(knitr)
library(kableExtra)

# Get unique values
seating_types <- sort(unique(na.omit(df$`Seating type`)))
areas <- sort(unique(na.omit(df$`CLUE small area`)))

# Pad the shorter list so both have equal length
max_length <- max(length(seating_types), length(areas))
seating_types <- c(seating_types, rep("", max_length - length(seating_types)))
areas <- c(areas, rep("", max_length - length(areas)))

# Combine into a transposed table
transposed_table <- rbind(seating_types, areas)
rownames(transposed_table) <- c("Seating Type", "CLUE Small Area")

# Display as wide table

kable(transposed_table, format = "html", caption = "Unique Values of Seating Type and CLUE Small Area") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"), full_width = F, position = "center")

```
</div>


As we can see from Table \@ref(tab:type), there are two types of seating group, with options like Indoor and Outdoor. The CLUE small area column shows many different areas in Melbourne, this allows us to compare business patterns by area.



```{r seating-boxplot-compact, fig.align = "center",fig.width=8, fig.height=2, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = " Distribution of Seating Capacity"}


library(ggplot2)

ggplot(df, aes(y = `Number of seats`)) +
  geom_boxplot(outlier.size = 0.8, fill = "skyblue") +
  coord_cartesian(ylim = c(0, 500)) +  # Zoom in to ignore extreme outliers
  labs(y = "Number of Seats") +
  theme_minimal(base_size = 10)



```

Number of Seats is the only continuous numerical variable available in our dataset for analysis. The Figure \@ref(fig:seating-boxplot-compact) shows that most businesses have fewer than 100 seats, but there are some with very high capacities. These large values are likely from big venues like stadiums or racecourses. The plot helps focus on typical businesses by zooming in and excluding extreme values.


```{r}


# Count occurrences of each industry type
industry_counts <- table(df$`Industry (ANZSIC4) description`)

# Sorted by frequency
sorted_industry_counts <- sort(industry_counts, decreasing = TRUE)


```




<div style="font-size: 80%;">
```{r  ind, message = FALSE, warning = FALSE, results = 'show'}
library(knitr)

# Convert and prepare data frame
industry_df <- as.data.frame(sorted_industry_counts)
colnames(industry_df) <- c("Industry_Type", "Count")



kable(head(industry_df, 8), format = "html", caption = "Table: Top 8 Industry Types by Number of Businesses") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"), full_width = F, position = "center")

```
</div>

The Table \@ref(tab:ind) shows the top 8 industry types in Melbourne’s hospitality sector based on the number of businesses recorded in the dataset. Cafes and Restaurants dominate the landscape with over 47,000 entries, followed by Takeaway Food Services and Pubs. Smaller but still notable categories include Accommodation, Bakeries, and Catering Services, highlighting the variety within the food and hospitality industry across Melbourne.

















#### Data transforming



```{r}






# Read raw data
raw_df <- read_csv("data/cafes-and-restaurants-with-seating-capacity.csv")


# Select industry




df_cleaned <- raw_df %>%
  filter(
    `Industry (ANZSIC4) description` %in% c(
      "Cafes and Restaurants",
      "Takeaway Food Services"
    )
  ) %>%
  filter(
    !is.na(`Trading name`),
    !is.na(`Property ID`),
    !is.na(`Census year`),
    str_trim(`Trading name`) != "",
    `Census year` >= 2002,
    `Census year` <= 2023
  ) %>%
  mutate(
    industry = case_when(
      `Industry (ANZSIC4) description` == "Cafes and Restaurants" ~ "Cafes",
      `Industry (ANZSIC4) description` == "Takeaway Food Services" ~ "Takeaway",
      TRUE ~ "Other"
    ),
    trading_name_original = str_trim(str_to_title(`Trading name`)),
    `Number of seats` = ifelse(is.na(`Number of seats`) | `Number of seats` <= 0, 1, `Number of seats`)
  )



# Show industry breakdown
industry_counts <- df_cleaned %>% count(industry)

print(industry_counts)



#  First Word Normalization function


get_normalized_first_word <- function(name) {
  if (is.na(name) || name == "") return(NA)
  
  # Convert to lowercase
  name <- str_to_lower(name)
  
  # Remove ALL punctuation
  name <- str_replace_all(name, "[^a-z0-9\\s]", " ")
  
  # Collapse multiple spaces
  name <- str_squish(name)
  
  # Split into words
  words <- str_split(name, "\\s+")[[1]]
  words <- words[words != ""]
  
  # Remove very short words
  words <- words[nchar(words) >= 3]
  
  if (length(words) == 0) return(NA)
  
  # Get first word
  first_word <- words[1]
  
  # Normalize singular/plural (remove trailing 's' from 5+ char words)
  if (nchar(first_word) >= 5 && str_ends(first_word, "s")) {
    singular <- str_sub(first_word, 1, -2)
    if (nchar(singular) >= 4) {
      first_word <- singular
    }
  }
  
  # Handle common variations
  first_word <- case_when(
    first_word %in% c("madam", "madame") ~ "madam",
    first_word %in% c("cafe", "caffee", "caffe") ~ "cafe",
    TRUE ~ first_word
  )
  
  return(first_word)
}


# Apply Normalization 




df_normalized <- df_cleaned %>%
  mutate(
    normalized_first_word = sapply(`Trading name`, get_normalized_first_word)
  ) %>%
  filter(!is.na(normalized_first_word), nchar(normalized_first_word) >= 2)




#  Consolidate seating types



df_consolidated <- df_normalized %>%
  group_by(`Census year`, `Property ID`, normalized_first_word) %>%
  summarise(
    trading_name = names(sort(table(trading_name_original), decreasing = TRUE))[1],
    building_address = first(`Building address`),
    business_address = first(`Business address`),
    clue_area = first(`CLUE small area`),
    industry = first(industry),
    lon = first(`Longitude`),
    lat = first(`Latitude`),
    seating_type = case_when(
      n_distinct(`Seating type`, na.rm = TRUE) == 1 ~ first(`Seating type`),
      TRUE ~ "Mixed"
    ),
    num_seats = sum(`Number of seats`, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  rename(
    census_year = `Census year`,
    property_id = `Property ID`
  )

cat("After seating consolidation:", nrow(df_consolidated), "records\n")



business_groups <- df_consolidated %>%
  group_by(property_id, normalized_first_word) %>%
  mutate(business_id = cur_group_id()) %>%
  ungroup()



# Add tempral metrics




df_with_lifespans <- business_groups %>%
  group_by(business_id) %>%
  arrange(census_year) %>%
  mutate(
    lifespan_years = max(census_year) - min(census_year) + 1,
    lifespan_group = case_when(
      lifespan_years <= 2 ~ "1-2 yrs",
      lifespan_years <= 5 ~ "3-5 yrs", 
      lifespan_years <= 8 ~ "6-8 yrs",
      lifespan_years <= 12 ~ "9-12 yrs",
      TRUE ~ ">12 yrs"
    )
  ) %>%
  ungroup()

# Add property metrics
property_counts <- df_with_lifespans %>%
  group_by(property_id, census_year) %>%
  summarise(businesses_at_property = n_distinct(business_id), .groups = "drop")

df_final <- df_with_lifespans %>%
  left_join(property_counts, by = c("property_id", "census_year")) %>%
  select(
    property_id, business_id, census_year, industry, clue_area, 
    trading_name, business_address, building_address, seating_type, 
    num_seats, lifespan_years, lifespan_group, lon, lat, businesses_at_property
  ) %>%
  arrange(census_year, property_id, business_id)



#  Fix remaining name variations



# Find potential name variations at same property
property_businesses <- df_final %>%
  group_by(property_id, business_id) %>%
  summarise(
    trading_name = first(trading_name),
    years = paste(min(census_year), max(census_year), sep = "-"),
    .groups = "drop"
  ) %>%
  group_by(property_id) %>%
  mutate(
    businesses_at_property = n(),
    name_simple = str_to_lower(str_remove_all(trading_name, "[^a-z0-9]"))
  ) %>%
  ungroup()

# Create merge map for similar names
merge_map <- list()
merge_count <- 0

properties_to_check <- unique(property_businesses$property_id[property_businesses$businesses_at_property >= 2])

for (prop_id in properties_to_check) {
  businesses <- property_businesses %>% filter(property_id == prop_id)
  
  if (nrow(businesses) < 2) next
  
  for (i in 1:(nrow(businesses) - 1)) {
    for (j in (i + 1):nrow(businesses)) {
      name1 <- businesses$name_simple[i]
      name2 <- businesses$name_simple[j]
      
      # Remove "the" prefix for comparison
      clean1 <- str_remove(name1, "^the")
      clean2 <- str_remove(name2, "^the")
      
      # Check if they should merge
      should_merge <- FALSE
      
      # Rule 1: Exact match after removing "the"
      if (clean1 == clean2) {
        should_merge <- TRUE
      }
      
      # Rule 2: One is subset with small difference
      else if (nchar(clean1) >= 5 && nchar(clean2) >= 5) {
        if (str_detect(clean1, fixed(clean2)) || str_detect(clean2, fixed(clean1))) {
          len_diff <- abs(nchar(clean1) - nchar(clean2))
          if (len_diff <= 10) {
            should_merge <- TRUE
          }
        }
      }
      
      if (should_merge) {
        id1 <- businesses$business_id[i]
        id2 <- businesses$business_id[j]
        
        # Merge to smaller ID
        if (id1 < id2) {
          merge_map[[as.character(id2)]] <- id1
        } else {
          merge_map[[as.character(id1)]] <- id2
        }
        merge_count <- merge_count + 1
      }
    }
  }
}



# Apply merges
get_final_id <- function(id) {
  while (!is.null(merge_map[[as.character(id)]])) {
    id <- merge_map[[as.character(id)]]
  }
  return(id)
}

df_ultra_final <- df_final %>%
  mutate(business_id = sapply(business_id, get_final_id))

# Recalculate metrics after merges
df_ultra_final <- df_ultra_final %>%
  group_by(business_id) %>%
  mutate(
    lifespan_years = max(census_year) - min(census_year) + 1,
    lifespan_group = case_when(
      lifespan_years <= 2 ~ "1-2 yrs",
      lifespan_years <= 5 ~ "3-5 yrs", 
      lifespan_years <= 8 ~ "6-8 yrs",
      lifespan_years <= 12 ~ "9-12 yrs",
      TRUE ~ ">12 yrs"
    )
  ) %>%
  ungroup()

# Recalculate property counts
property_counts_final <- df_ultra_final %>%
  group_by(property_id, census_year) %>%
  summarise(businesses_at_property = n_distinct(business_id), .groups = "drop")

df_ultra_final <- df_ultra_final %>%
  select(-businesses_at_property) %>%
  left_join(property_counts_final, by = c("property_id", "census_year")) %>%
  select(
    property_id, business_id, census_year, industry, clue_area, 
    trading_name, business_address, building_address, seating_type, 
    num_seats, lifespan_years, lifespan_group, lon, lat, businesses_at_property
  ) %>%
  arrange(census_year, property_id, business_id)




# SAVE 




write_csv(df_ultra_final, "data/df_FINAL_2industries.csv")


# Create business summary
business_summary <- df_ultra_final %>%
  group_by(property_id, business_id) %>%
  summarise(
    trading_name = first(trading_name),
    first_year = min(census_year),
    last_year = max(census_year),
    years_active = n_distinct(census_year),
    industry = first(industry),
    .groups = "drop"
  ) %>%
  mutate(
    year_span = ifelse(first_year == last_year, as.character(first_year), 
                      paste0(first_year, "-", last_year))
  ) %>%
  group_by(property_id) %>%
  mutate(businesses_at_property = n()) %>%
  ungroup() %>%
  select(property_id, business_id, trading_name, year_span, years_active, 
         industry, businesses_at_property) %>%
  arrange(property_id, year_span)

write_csv(business_summary, "data/business_summary_ULTRA_FINAL_2industries.csv")


# SUMMARY




# Industry breakdown
industry_final <- df_ultra_final %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  count(industry)


print(industry_final)

# Lifespan distribution
lifespan_dist <- df_ultra_final %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  count(lifespan_group) %>%
  mutate(percentage = round(n / sum(n) * 100, 1))


print(lifespan_dist)


```







```{r}



# Load the Ultra final data
df_ultra <- read_csv("data/df_FINAL_2industries.csv")


# Calculate business operating status 


business_status <- df_ultra %>%
  group_by(business_id) %>%
  summarise(
    business_start_year = min(census_year),
    business_end_year = max(census_year),
    still_operating = (business_end_year == 2023),  
    lifespan_group = first(lifespan_group),
    .groups = "drop"
  )


# ADD bias filters

enhanced_ultra <- df_ultra %>%
  left_join(business_status %>% select(business_id, business_start_year, business_end_year, still_operating), 
            by = "business_id") %>%
  mutate(

    # TRUE = exclude from bias-corrected analysis (but keep for temporal analysis)
    # FALSE = include in all analysis
    bias_filter = case_when(
      # Keep ALL >12 year businesses (whether operating or closed)
      lifespan_group == ">12 yrs" ~ FALSE,
      
      # For shorter lifespan groups: only keep closed businesses
      lifespan_group != ">12 yrs" & !still_operating ~ FALSE,  # Closed = keep (true lifespan)
      lifespan_group != ">12 yrs" & still_operating ~ TRUE,    # Still operating = filter out (biased lifespan)
      
      TRUE ~ FALSE
    ),
    
    # Add descriptive status for transparency
    business_status = case_when(
      lifespan_group == ">12 yrs" & still_operating ~ "Long-term Success (Operating)",
      lifespan_group == ">12 yrs" & !still_operating ~ "Long-term Success (Closed)",
      !still_operating ~ "Closed Business",
      still_operating ~ "Still Operating (Biased Lifespan)",
      TRUE ~ "Unknown"
    ),
    
    # Add analysis recommendation
    analysis_usage = case_when(
      bias_filter == FALSE ~ "Use in All Analysis",
      bias_filter == TRUE ~ "Temporal Only (Exclude from Lifespan Analysis)",
      TRUE ~ "Unknown"
    )
  ) %>%
  select(-business_start_year, -business_end_year, -still_operating) %>%
  arrange(census_year, property_id, business_id)

# Check quality



# Overall filtering impact
total_records <- nrow(enhanced_ultra)
filtered_records <- sum(enhanced_ultra$bias_filter)
kept_records <- total_records - filtered_records



# Business-level analysis
business_analysis <- enhanced_ultra %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  count(lifespan_group, business_status) %>%
  pivot_wider(names_from = business_status, values_from = n, values_fill = 0)


print(business_analysis)

# Lifespan group impact
lifespan_impact <- enhanced_ultra %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  group_by(lifespan_group) %>%
  summarise(
    total_businesses = n(),
    filtered_out = sum(bias_filter),
    kept_for_analysis = total_businesses - filtered_out,
    filter_rate = round(filtered_out / total_businesses * 100, 1),
    .groups = "drop"
  ) %>%
  arrange(lifespan_group)


print(lifespan_impact)

# Area impact analysis
area_impact <- enhanced_ultra %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  group_by(clue_area) %>%
  summarise(
    total_businesses = n(),
    filtered_out = sum(bias_filter),
    kept_for_analysis = total_businesses - filtered_out,
    filter_rate = round(filtered_out / total_businesses * 100, 1),
    .groups = "drop"
  ) %>%
  arrange(desc(total_businesses))

cat("\n=== Impact by Area (Top 10) ===\n")
print(head(area_impact, 10))

# Temporal impact by year
temporal_impact <- enhanced_ultra %>%
  group_by(census_year) %>%
  summarise(
    total_records = n(),
    filtered_records = sum(bias_filter),
    kept_records = total_records - filtered_records,
    filter_rate = round(filtered_records / total_records * 100, 1),
    .groups = "drop"
  )


print(tail(temporal_impact, 8))

# Industry impact
industry_impact <- enhanced_ultra %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  group_by(industry) %>%
  summarise(
    total_businesses = n(),
    filtered_out = sum(bias_filter),
    kept_for_analysis = total_businesses - filtered_out,
    filter_rate = round(filtered_out / total_businesses * 100, 1),
    .groups = "drop"
  )


print(industry_impact)









# SAVE ENHANCED DATA


output_file <- "data/df_enhanced_with_filters.csv"
write_csv(enhanced_ultra, output_file)




```




#### Add new features

```{r}



df <- read_csv("data/df_enhanced_with_filters.csv", show_col_types = FALSE)


# Full street types 
street_types_full <- c(
  "Boulevard","Parade","Avenue","Street","Road","Lane","Place","Court","Drive","Walk",
  "Terrace","Highway","Way","Crescent","Square","Mall","Arcade","Circuit","Grove","Close",
  "Esplanade","Promenade","Circus","Quay","Alley","Market","Bridge","Freeway","Causeway",
  "Broadway","Link","Row","Mews","Path","Steps","Rise","Track","Expressway","Bypass"
)

# Abbreviations to expand AFTER we match
abbr_map <- c(
  "\\bSt\\b"  = "Street",
  "\\bRd\\b"  = "Road",
  "\\bAve\\b" = "Avenue",
  "\\bLn\\b"  = "Lane",
  "\\bPde\\b" = "Parade",
  "\\bBlvd\\b"= "Boulevard",
  "\\bBvd\\b" = "Boulevard",
  "\\bPl\\b"  = "Place",
  "\\bCt\\b"  = "Court",
  "\\bDr\\b"  = "Drive",
  "\\bTce\\b" = "Terrace",
  "\\bCres\\b"= "Crescent",
  "\\bEspl\\b"= "Esplanade",
  "\\bHwy\\b" = "Highway",
  "\\bWy\\b"  = "Way",
  "\\bArc\\b" = "Arcade"
)

# Accept either full types OR common abbreviations 
types_alt_with_abbr <- paste(
  c(street_types_full,
    "St","Rd","Ave","Ln","Pde","Blvd","Bvd","Pl","Ct","Dr","Tce","Cres","Espl","Hwy","Wy","Arc"),
  collapse = "|"
)

# Drop common unit/building prefixes at the start
prefix_re <- regex(
  "^(Shop|Unit|Suite|Level|Lvl|Ground|Floor|Tenancy|Kiosk|Building|Bldg|Block|Office|Room|Rm|Apt|Apartment|Podium|Mezzanine)\\s*[A-Za-z0-9\\-/]*\\s*,?\\s*",
  ignore_case = TRUE
)

# Core extractor 
extract_street <- function(address) {
  if (is.na(address) || str_trim(address) == "") return(NA_character_)

  # Normalise + remove unit/building prefixes
  addr <- str_squish(address)
  addr <- str_remove(addr, prefix_re)

  # Pattern: "<Name tokens> <Type|Abbrev>"
  # Works for "Little Latrobe St", "Riverside Quay", "Collins Street", etc.
  pattern <- paste0("([A-Za-z][A-Za-z\\s'\\-]*?)\\s+(", types_alt_with_abbr, ")\\b")

  m <- str_extract(addr, regex(pattern, ignore_case = TRUE))

  # If not found, retry within comma-separated parts (helps with locality suffixes)
  if (is.na(m) && str_detect(addr, ",")) {
    parts <- str_split(addr, ",")[[1]]
    for (p in parts) {
      mm <- str_extract(str_squish(p), regex(pattern, ignore_case = TRUE))
      if (!is.na(mm)) { m <- mm; break }
    }
  }
  if (is.na(m)) return(NA_character_)

  # Remove leading articles OR single-letter tokens (e.g., "A Bourke Street")
  m <- str_remove(m, regex("^(?i)(a|an|the)\\s+"))
  m <- str_remove(m, regex("^([A-Z])\\s+(?=[A-Za-z])"))

  # Expand abbreviations inside the matched phrase
  for (pat in names(abbr_map)) {
    m <- str_replace_all(m, regex(pat, ignore_case = TRUE), abbr_map[[pat]])
  }

  # Title case and tidy
  str_to_title(str_squish(m))
}

# Apply + derive street_type 
df_with_streets <- df %>%
  mutate(
    street_address = map_chr(business_address, extract_street),
    street_type    = str_extract(street_address, paste(street_types_full, collapse = "|"))
  )



# Save outputs 
write_csv(df_with_streets, "data/df_with_streets.csv")





```
















```{r}



df <- read_csv("data/df_with_streets.csv", show_col_types = FALSE)

# replacement in street_address
df$street_address[df$street_address == "Alfred Deakin Building Federation Square"] <- "Federation Square"
df$street_address[df$street_address == "Cafe Royal Botanic Gardens Alexandra Avenue"] <- "Alexandra Avenue"
df$street_address[df$street_address == "Montefiore House Exhibition Mews"] <- "Exhibition Mews"
df$street_address[df$street_address == "Restaurant Melbourne Zoological Gardens Elliott Avenue"] <- "Elliott Avenue"
df$street_address[df$street_address == "Restaurant Old Observatory Building Royal Botanic Gardens Birdwood Avenue"] <- "Birdwood Avenue"
df$street_address[df$street_address == "Shed F Queen Victoria Market"] <- "Victoria Market"
df$street_address[df$street_address == "Shed M Queen Victoria Market"] <- "Victoria Market"
df$street_address[df$street_address == "Square Federation Square"] <- "Federation Square"
df$street_address[df$street_address == "Street Kilda Road"] <- "St Kilda Road"
df$street_address[df$street_address == "Street Andrews Place"] <- "St Andrews Place"
df$street_address[df$street_address == "Tea Rooms Royal Botanic Gardens Alexandra Avenue"] <- "Alexandra Avenue"
df$street_address[df$street_address == "Yarra Building Federation Square"] <- "Federation Square"
df$street_address[df$street_address == "Zoological Gardens Elliott Avenue"] <- "Elliott Avenue"

# Remove street_type 
df <- df %>% select(-any_of("street_type"))

# Save
write_csv(df, "data/df_with_streets.csv")

# Also save updated unique streets
unique_streets <- df %>%
  filter(!is.na(street_address)) %>%
  distinct(street_address) %>%
  arrange(street_address)

write_csv(unique_streets, "data/unique_streets.csv")




```













































































































