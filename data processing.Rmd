---
title: "Intership project"
output: html_document
date: "2025-09-27"
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,        
  eval = TRUE,         
  cache = FALSE,
  warning = FALSE,     
  message = FALSE,    
  results = 'hide'     
)
options(width = 80, digits = 3)


library(dplyr)
library(tidyverse)
library(here)
library(galah)
library(sf)
library(spData)
library(ggplot2)
library(Cairo)
library(kableExtra)
library(bookdown)
library(tibble)
library(purrr)
library(knitr)
library(readr)
library(stringr)
library(purrr)
```





#### Data description

The ‘Cafes and Restaurants with Seating Capacity’ dataset is sourced from the City of Melbourne’s Census of Land Use and Employment (CLUE). It provides annual records of business establishments located within the Melbourne local government area from 2002 to 2023. The data can be retrieved via the [City of Melbourne Open Data Portal](https://data.melbourne.vic.gov.au/explore/dataset/cafes-and-restaurants-with-seating-capacity/information).


<div style="font-size: 80%;">
```{r var-types,message = FALSE, warning = FALSE, results = 'show'}
# Read the cleaned CSV files into R environment
df <- read_csv("data/cafes-and-restaurants-with-seating-capacity.csv")

# Get column names and types
col_types <- map_chr(df, ~ class(.x)[1])
# Split into two groups
first_half <- col_types[1:7]
second_half <- col_types[8:15]
# Build a 2-row tibble with padding between groups
df_split_summary <- tibble::tibble(
  !!!setNames(as.list(first_half), names(first_half)),
  !!!setNames(as.list(second_half), names(second_half))
)
# Use HTML-based styling but ensure it's compatible with PDF conversion
kbl(df_split_summary, format = "html", caption = "Variable Types in CLUE Dataset", 
    table.attr = 'id="tab:var-types"') %>% # Add explicit ID
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"), 
                full_width = FALSE, 
                position = "center")
```
</div>


The Table \@ref(tab:var-types) shows the column names and data types in the dataset. It includes numbers, text, and location data . This mix of data types makes it useful for different kinds of analysis. The variable names in the dataset are generally clear and descriptive.






<div style="font-size: 80%;">
```{r  summary, message = FALSE, warning = FALSE, results = 'show'}


# Dimensions
n_rows <- nrow(df)
n_cols <- ncol(df)

# Duplicates
n_duplicates <- sum(duplicated(df))

# Missing values
missing_summary <- colSums(is.na(df))
total_missing <- sum(missing_summary)
missing_cols <- names(missing_summary[missing_summary > 0])
missing_cols_str <- paste(missing_cols, collapse = ", ")

# Negative or zero seating values
invalid_seating <- sum(df$`Number of seats` < 0, na.rm = TRUE)

# Create horizontal summary table
df_summary <- tibble(
  `Number of Rows` = n_rows,
  `Number of Columns` = n_cols,
  `Number of Duplicate Rows` = n_duplicates,
  `Total Missing Values` = total_missing,
  `Columns with NA Values` = length(missing_cols),
  `Names of Columns with NA Values` = missing_cols_str,
  `Non-positive Seating Values` = invalid_seating
)

# Display the summary table with border and proper alignment
kable(df_summary, format = "html", caption = "Summary of Dataset Structure and Quality Checks") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"), full_width = F, position = "center")



```
</div>





As shown in Table \@ref(tab:summary), the dataset has 63,121 rows and 15 columns, with no duplicate entries and negative seating number. There are 1,581 missing values, all in the location-related columns (Longitude, Latitude, location). 


<div style="font-size: 80%;">
```{r year, message = FALSE, warning = FALSE, results = 'show'}

library(tibble)
library(knitr)

# Get sorted unique census years
census_years <- sort(unique(df$`Census year`))

# Create one-row, one-cell table
census_table <- tibble(`Census Years` = paste(census_years, collapse = ", "))


kable(census_table, format = "html", caption = "Unique Census Years in the Dataset") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"), full_width = F)

```
</div>

Table \@ref(tab:year) shows the dataset covers 22 unique census years, ranging from 2002 to 2023. 


<div style="font-size: 80%;">
```{r  type, message = FALSE, warning = FALSE, results = 'show'}
library(dplyr)
library(knitr)
library(kableExtra)

library(dplyr)
library(knitr)
library(kableExtra)

# Get unique values
seating_types <- sort(unique(na.omit(df$`Seating type`)))
areas <- sort(unique(na.omit(df$`CLUE small area`)))

# Pad the shorter list so both have equal length
max_length <- max(length(seating_types), length(areas))
seating_types <- c(seating_types, rep("", max_length - length(seating_types)))
areas <- c(areas, rep("", max_length - length(areas)))

# Combine into a transposed table
transposed_table <- rbind(seating_types, areas)
rownames(transposed_table) <- c("Seating Type", "CLUE Small Area")

# Display as wide table

kable(transposed_table, format = "html", caption = "Unique Values of Seating Type and CLUE Small Area") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"), full_width = F, position = "center")

```
</div>


As we can see from Table \@ref(tab:type), there are two types of seating group, with options like Indoor and Outdoor. The CLUE small area column shows many different areas in Melbourne, this allows us to compare business patterns by area.



```{r seating-boxplot-compact, fig.align = "center",fig.width=8, fig.height=2, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = " Distribution of Seating Capacity"}


library(ggplot2)

ggplot(df, aes(y = `Number of seats`)) +
  geom_boxplot(outlier.size = 0.8, fill = "skyblue") +
  coord_cartesian(ylim = c(0, 500)) +  # Zoom in to ignore extreme outliers
  labs(y = "Number of Seats") +
  theme_minimal(base_size = 10)



```

Number of Seats is the only continuous numerical variable available in our dataset for analysis. The Figure \@ref(fig:seating-boxplot-compact) shows that most businesses have fewer than 100 seats, but there are some with very high capacities. These large values are likely from big venues like stadiums or racecourses. The plot helps focus on typical businesses by zooming in and excluding extreme values.


```{r}


# Count occurrences of each industry type
industry_counts <- table(df$`Industry (ANZSIC4) description`)

# Sorted by frequency
sorted_industry_counts <- sort(industry_counts, decreasing = TRUE)


```




<div style="font-size: 80%;">
```{r  ind, message = FALSE, warning = FALSE, results = 'show'}
library(knitr)

# Convert and prepare data frame
industry_df <- as.data.frame(sorted_industry_counts)
colnames(industry_df) <- c("Industry_Type", "Count")



kable(head(industry_df, 8), format = "html", caption = "Table: Top 8 Industry Types by Number of Businesses") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "condensed"), full_width = F, position = "center")

```
</div>

The Table \@ref(tab:ind) shows the top 8 industry types in Melbourne’s hospitality sector based on the number of businesses recorded in the dataset. Cafes and Restaurants dominate the landscape with over 47,000 entries, followed by Takeaway Food Services and Pubs. Smaller but still notable categories include Accommodation, Bakeries, and Catering Services, highlighting the variety within the food and hospitality industry across Melbourne.

















#### Data transforming



```{r}






# Read raw data
raw_df <- read_csv("data/cafes-and-restaurants-with-seating-capacity.csv")


# Select industry




df_cleaned <- raw_df %>%
  filter(
    `Industry (ANZSIC4) description` %in% c(
      "Cafes and Restaurants",
      "Takeaway Food Services"
    )
  ) %>%
  filter(
    !is.na(`Trading name`),
    !is.na(`Property ID`),
    !is.na(`Census year`),
    str_trim(`Trading name`) != "",
    `Census year` >= 2002,
    `Census year` <= 2023
  ) %>%
  mutate(
    industry = case_when(
      `Industry (ANZSIC4) description` == "Cafes and Restaurants" ~ "Cafes",
      `Industry (ANZSIC4) description` == "Takeaway Food Services" ~ "Takeaway",
      TRUE ~ "Other"
    ),
    trading_name_original = str_trim(str_to_title(`Trading name`)),
    `Number of seats` = ifelse(is.na(`Number of seats`) | `Number of seats` <= 0, 1, `Number of seats`)
  )



# Show industry breakdown
industry_counts <- df_cleaned %>% count(industry)

print(industry_counts)



#  First Word Normalization function


get_normalized_first_word <- function(name) {
  if (is.na(name) || name == "") return(NA)
  
  # Convert to lowercase
  name <- str_to_lower(name)
  
  # Remove ALL punctuation
  name <- str_replace_all(name, "[^a-z0-9\\s]", " ")
  
  # Collapse multiple spaces
  name <- str_squish(name)
  
  # Split into words
  words <- str_split(name, "\\s+")[[1]]
  words <- words[words != ""]
  
  # Remove very short words
  words <- words[nchar(words) >= 3]
  
  if (length(words) == 0) return(NA)
  
  # Get first word
  first_word <- words[1]
  
  # Normalize singular/plural (remove trailing 's' from 5+ char words)
  if (nchar(first_word) >= 5 && str_ends(first_word, "s")) {
    singular <- str_sub(first_word, 1, -2)
    if (nchar(singular) >= 4) {
      first_word <- singular
    }
  }
  
  # Handle common variations
  first_word <- case_when(
    first_word %in% c("madam", "madame") ~ "madam",
    first_word %in% c("cafe", "caffee", "caffe") ~ "cafe",
    TRUE ~ first_word
  )
  
  return(first_word)
}


# Apply Normalization 




df_normalized <- df_cleaned %>%
  mutate(
    normalized_first_word = sapply(`Trading name`, get_normalized_first_word)
  ) %>%
  filter(!is.na(normalized_first_word), nchar(normalized_first_word) >= 2)




#  Consolidate seating types



df_consolidated <- df_normalized %>%
  group_by(`Census year`, `Property ID`, normalized_first_word) %>%
  summarise(
    trading_name = names(sort(table(trading_name_original), decreasing = TRUE))[1],
    building_address = first(`Building address`),
    business_address = first(`Business address`),
    clue_area = first(`CLUE small area`),
    industry = first(industry),
    lon = first(`Longitude`),
    lat = first(`Latitude`),
    seating_type = case_when(
      n_distinct(`Seating type`, na.rm = TRUE) == 1 ~ first(`Seating type`),
      TRUE ~ "Mixed"
    ),
    num_seats = sum(`Number of seats`, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  rename(
    census_year = `Census year`,
    property_id = `Property ID`
  )

cat("After seating consolidation:", nrow(df_consolidated), "records\n")



business_groups <- df_consolidated %>%
  group_by(property_id, normalized_first_word) %>%
  mutate(business_id = cur_group_id()) %>%
  ungroup()



# Add tempral metrics




df_with_lifespans <- business_groups %>%
  group_by(business_id) %>%
  arrange(census_year) %>%
  mutate(
    lifespan_years = max(census_year) - min(census_year) + 1,
    lifespan_group = case_when(
      lifespan_years <= 2 ~ "1-2 yrs",
      lifespan_years <= 5 ~ "3-5 yrs", 
      lifespan_years <= 8 ~ "6-8 yrs",
      lifespan_years <= 12 ~ "9-12 yrs",
      TRUE ~ ">12 yrs"
    )
  ) %>%
  ungroup()

# Add property metrics
property_counts <- df_with_lifespans %>%
  group_by(property_id, census_year) %>%
  summarise(businesses_at_property = n_distinct(business_id), .groups = "drop")

df_final <- df_with_lifespans %>%
  left_join(property_counts, by = c("property_id", "census_year")) %>%
  select(
    property_id, business_id, census_year, industry, clue_area, 
    trading_name, business_address, building_address, seating_type, 
    num_seats, lifespan_years, lifespan_group, lon, lat, businesses_at_property
  ) %>%
  arrange(census_year, property_id, business_id)



#  Fix remaining name variations



# Find potential name variations at same property
property_businesses <- df_final %>%
  group_by(property_id, business_id) %>%
  summarise(
    trading_name = first(trading_name),
    years = paste(min(census_year), max(census_year), sep = "-"),
    .groups = "drop"
  ) %>%
  group_by(property_id) %>%
  mutate(
    businesses_at_property = n(),
    name_simple = str_to_lower(str_remove_all(trading_name, "[^a-z0-9]"))
  ) %>%
  ungroup()

# Create merge map for similar names
merge_map <- list()
merge_count <- 0

properties_to_check <- unique(property_businesses$property_id[property_businesses$businesses_at_property >= 2])

for (prop_id in properties_to_check) {
  businesses <- property_businesses %>% filter(property_id == prop_id)
  
  if (nrow(businesses) < 2) next
  
  for (i in 1:(nrow(businesses) - 1)) {
    for (j in (i + 1):nrow(businesses)) {
      name1 <- businesses$name_simple[i]
      name2 <- businesses$name_simple[j]
      
      # Remove "the" prefix for comparison
      clean1 <- str_remove(name1, "^the")
      clean2 <- str_remove(name2, "^the")
      
      # Check if they should merge
      should_merge <- FALSE
      
      # Rule 1: Exact match after removing "the"
      if (clean1 == clean2) {
        should_merge <- TRUE
      }
      
      # Rule 2: One is subset with small difference
      else if (nchar(clean1) >= 5 && nchar(clean2) >= 5) {
        if (str_detect(clean1, fixed(clean2)) || str_detect(clean2, fixed(clean1))) {
          len_diff <- abs(nchar(clean1) - nchar(clean2))
          if (len_diff <= 10) {
            should_merge <- TRUE
          }
        }
      }
      
      if (should_merge) {
        id1 <- businesses$business_id[i]
        id2 <- businesses$business_id[j]
        
        # Merge to smaller ID
        if (id1 < id2) {
          merge_map[[as.character(id2)]] <- id1
        } else {
          merge_map[[as.character(id1)]] <- id2
        }
        merge_count <- merge_count + 1
      }
    }
  }
}



# Apply merges
get_final_id <- function(id) {
  while (!is.null(merge_map[[as.character(id)]])) {
    id <- merge_map[[as.character(id)]]
  }
  return(id)
}

df_ultra_final <- df_final %>%
  mutate(business_id = sapply(business_id, get_final_id))

# Recalculate metrics after merges
df_ultra_final <- df_ultra_final %>%
  group_by(business_id) %>%
  mutate(
    lifespan_years = max(census_year) - min(census_year) + 1,
    lifespan_group = case_when(
      lifespan_years <= 2 ~ "1-2 yrs",
      lifespan_years <= 5 ~ "3-5 yrs", 
      lifespan_years <= 8 ~ "6-8 yrs",
      lifespan_years <= 12 ~ "9-12 yrs",
      TRUE ~ ">12 yrs"
    )
  ) %>%
  ungroup()

# Recalculate property counts
property_counts_final <- df_ultra_final %>%
  group_by(property_id, census_year) %>%
  summarise(businesses_at_property = n_distinct(business_id), .groups = "drop")

df_ultra_final <- df_ultra_final %>%
  select(-businesses_at_property) %>%
  left_join(property_counts_final, by = c("property_id", "census_year")) %>%
  select(
    property_id, business_id, census_year, industry, clue_area, 
    trading_name, business_address, building_address, seating_type, 
    num_seats, lifespan_years, lifespan_group, lon, lat, businesses_at_property
  ) %>%
  arrange(census_year, property_id, business_id)




# SAVE 




write_csv(df_ultra_final, "data/df_FINAL_2industries.csv")


# Create business summary
business_summary <- df_ultra_final %>%
  group_by(property_id, business_id) %>%
  summarise(
    trading_name = first(trading_name),
    first_year = min(census_year),
    last_year = max(census_year),
    years_active = n_distinct(census_year),
    industry = first(industry),
    .groups = "drop"
  ) %>%
  mutate(
    year_span = ifelse(first_year == last_year, as.character(first_year), 
                      paste0(first_year, "-", last_year))
  ) %>%
  group_by(property_id) %>%
  mutate(businesses_at_property = n()) %>%
  ungroup() %>%
  select(property_id, business_id, trading_name, year_span, years_active, 
         industry, businesses_at_property) %>%
  arrange(property_id, year_span)

write_csv(business_summary, "data/business_summary_ULTRA_FINAL_2industries.csv")


# SUMMARY




# Industry breakdown
industry_final <- df_ultra_final %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  count(industry)


print(industry_final)

# Lifespan distribution
lifespan_dist <- df_ultra_final %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  count(lifespan_group) %>%
  mutate(percentage = round(n / sum(n) * 100, 1))


print(lifespan_dist)


```







```{r}



# Load the Ultra final data
df_ultra <- read_csv("data/df_FINAL_2industries.csv")


# Calculate business operating status 


business_status <- df_ultra %>%
  group_by(business_id) %>%
  summarise(
    business_start_year = min(census_year),
    business_end_year = max(census_year),
    still_operating = (business_end_year == 2023),  
    lifespan_group = first(lifespan_group),
    .groups = "drop"
  )


# ADD bias filters

enhanced_ultra <- df_ultra %>%
  left_join(business_status %>% select(business_id, business_start_year, business_end_year, still_operating), 
            by = "business_id") %>%
  mutate(

    # TRUE = exclude from bias-corrected analysis (but keep for temporal analysis)
    # FALSE = include in all analysis
    bias_filter = case_when(
      # Keep ALL >12 year businesses (whether operating or closed)
      lifespan_group == ">12 yrs" ~ FALSE,
      
      # For shorter lifespan groups: only keep closed businesses
      lifespan_group != ">12 yrs" & !still_operating ~ FALSE,  # Closed = keep (true lifespan)
      lifespan_group != ">12 yrs" & still_operating ~ TRUE,    # Still operating = filter out (biased lifespan)
      
      TRUE ~ FALSE
    ),
    
    # Add descriptive status for transparency
    business_status = case_when(
      lifespan_group == ">12 yrs" & still_operating ~ "Long-term Success (Operating)",
      lifespan_group == ">12 yrs" & !still_operating ~ "Long-term Success (Closed)",
      !still_operating ~ "Closed Business",
      still_operating ~ "Still Operating (Biased Lifespan)",
      TRUE ~ "Unknown"
    ),
    
    # Add analysis recommendation
    analysis_usage = case_when(
      bias_filter == FALSE ~ "Use in All Analysis",
      bias_filter == TRUE ~ "Temporal Only (Exclude from Lifespan Analysis)",
      TRUE ~ "Unknown"
    )
  ) %>%
  select(-business_start_year, -business_end_year, -still_operating) %>%
  arrange(census_year, property_id, business_id)

# Check quality



# Overall filtering impact
total_records <- nrow(enhanced_ultra)
filtered_records <- sum(enhanced_ultra$bias_filter)
kept_records <- total_records - filtered_records



# Business-level analysis
business_analysis <- enhanced_ultra %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  count(lifespan_group, business_status) %>%
  pivot_wider(names_from = business_status, values_from = n, values_fill = 0)


print(business_analysis)

# Lifespan group impact
lifespan_impact <- enhanced_ultra %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  group_by(lifespan_group) %>%
  summarise(
    total_businesses = n(),
    filtered_out = sum(bias_filter),
    kept_for_analysis = total_businesses - filtered_out,
    filter_rate = round(filtered_out / total_businesses * 100, 1),
    .groups = "drop"
  ) %>%
  arrange(lifespan_group)


print(lifespan_impact)

# Area impact analysis
area_impact <- enhanced_ultra %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  group_by(clue_area) %>%
  summarise(
    total_businesses = n(),
    filtered_out = sum(bias_filter),
    kept_for_analysis = total_businesses - filtered_out,
    filter_rate = round(filtered_out / total_businesses * 100, 1),
    .groups = "drop"
  ) %>%
  arrange(desc(total_businesses))

cat("\n=== Impact by Area (Top 10) ===\n")
print(head(area_impact, 10))

# Temporal impact by year
temporal_impact <- enhanced_ultra %>%
  group_by(census_year) %>%
  summarise(
    total_records = n(),
    filtered_records = sum(bias_filter),
    kept_records = total_records - filtered_records,
    filter_rate = round(filtered_records / total_records * 100, 1),
    .groups = "drop"
  )


print(tail(temporal_impact, 8))

# Industry impact
industry_impact <- enhanced_ultra %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  group_by(industry) %>%
  summarise(
    total_businesses = n(),
    filtered_out = sum(bias_filter),
    kept_for_analysis = total_businesses - filtered_out,
    filter_rate = round(filtered_out / total_businesses * 100, 1),
    .groups = "drop"
  )


print(industry_impact)









# SAVE ENHANCED DATA


output_file <- "data/df_enhanced_with_filters.csv"
write_csv(enhanced_ultra, output_file)




```




#### Add new features

```{r}



df <- read_csv("data/df_enhanced_with_filters.csv", show_col_types = FALSE)


# Full street types 
street_types_full <- c(
  "Boulevard","Parade","Avenue","Street","Road","Lane","Place","Court","Drive","Walk",
  "Terrace","Highway","Way","Crescent","Square","Mall","Arcade","Circuit","Grove","Close",
  "Esplanade","Promenade","Circus","Quay","Alley","Market","Bridge","Freeway","Causeway",
  "Broadway","Link","Row","Mews","Path","Steps","Rise","Track","Expressway","Bypass"
)

# Abbreviations to expand AFTER we match
abbr_map <- c(
  "\\bSt\\b"  = "Street",
  "\\bRd\\b"  = "Road",
  "\\bAve\\b" = "Avenue",
  "\\bLn\\b"  = "Lane",
  "\\bPde\\b" = "Parade",
  "\\bBlvd\\b"= "Boulevard",
  "\\bBvd\\b" = "Boulevard",
  "\\bPl\\b"  = "Place",
  "\\bCt\\b"  = "Court",
  "\\bDr\\b"  = "Drive",
  "\\bTce\\b" = "Terrace",
  "\\bCres\\b"= "Crescent",
  "\\bEspl\\b"= "Esplanade",
  "\\bHwy\\b" = "Highway",
  "\\bWy\\b"  = "Way",
  "\\bArc\\b" = "Arcade"
)

# Accept either full types OR common abbreviations 
types_alt_with_abbr <- paste(
  c(street_types_full,
    "St","Rd","Ave","Ln","Pde","Blvd","Bvd","Pl","Ct","Dr","Tce","Cres","Espl","Hwy","Wy","Arc"),
  collapse = "|"
)

# Drop common unit/building prefixes at the start
prefix_re <- regex(
  "^(Shop|Unit|Suite|Level|Lvl|Ground|Floor|Tenancy|Kiosk|Building|Bldg|Block|Office|Room|Rm|Apt|Apartment|Podium|Mezzanine)\\s*[A-Za-z0-9\\-/]*\\s*,?\\s*",
  ignore_case = TRUE
)

# Core extractor 
extract_street <- function(address) {
  if (is.na(address) || str_trim(address) == "") return(NA_character_)

  # Normalise + remove unit/building prefixes
  addr <- str_squish(address)
  addr <- str_remove(addr, prefix_re)

  # Pattern: "<Name tokens> <Type|Abbrev>"
  # Works for "Little Latrobe St", "Riverside Quay", "Collins Street", etc.
  pattern <- paste0("([A-Za-z][A-Za-z\\s'\\-]*?)\\s+(", types_alt_with_abbr, ")\\b")

  m <- str_extract(addr, regex(pattern, ignore_case = TRUE))

  # If not found, retry within comma-separated parts (helps with locality suffixes)
  if (is.na(m) && str_detect(addr, ",")) {
    parts <- str_split(addr, ",")[[1]]
    for (p in parts) {
      mm <- str_extract(str_squish(p), regex(pattern, ignore_case = TRUE))
      if (!is.na(mm)) { m <- mm; break }
    }
  }
  if (is.na(m)) return(NA_character_)

  # Remove leading articles OR single-letter tokens (e.g., "A Bourke Street")
  m <- str_remove(m, regex("^(?i)(a|an|the)\\s+"))
  m <- str_remove(m, regex("^([A-Z])\\s+(?=[A-Za-z])"))

  # Expand abbreviations inside the matched phrase
  for (pat in names(abbr_map)) {
    m <- str_replace_all(m, regex(pat, ignore_case = TRUE), abbr_map[[pat]])
  }

  # Title case and tidy
  str_to_title(str_squish(m))
}

# Apply + derive street_type 
df_with_streets <- df %>%
  mutate(
    street_address = map_chr(business_address, extract_street),
    street_type    = str_extract(street_address, paste(street_types_full, collapse = "|"))
  )



# Save outputs 
write_csv(df_with_streets, "data/df_with_streets.csv")





```
















```{r}



df <- read_csv("data/df_with_streets.csv", show_col_types = FALSE)

# replacement in street_address
df$street_address[df$street_address == "Alfred Deakin Building Federation Square"] <- "Federation Square"
df$street_address[df$street_address == "Cafe Royal Botanic Gardens Alexandra Avenue"] <- "Alexandra Avenue"
df$street_address[df$street_address == "Montefiore House Exhibition Mews"] <- "Exhibition Mews"
df$street_address[df$street_address == "Restaurant Melbourne Zoological Gardens Elliott Avenue"] <- "Elliott Avenue"
df$street_address[df$street_address == "Restaurant Old Observatory Building Royal Botanic Gardens Birdwood Avenue"] <- "Birdwood Avenue"
df$street_address[df$street_address == "Shed F Queen Victoria Market"] <- "Victoria Market"
df$street_address[df$street_address == "Shed M Queen Victoria Market"] <- "Victoria Market"
df$street_address[df$street_address == "Square Federation Square"] <- "Federation Square"
df$street_address[df$street_address == "Street Kilda Road"] <- "St Kilda Road"
df$street_address[df$street_address == "Street Andrews Place"] <- "St Andrews Place"
df$street_address[df$street_address == "Tea Rooms Royal Botanic Gardens Alexandra Avenue"] <- "Alexandra Avenue"
df$street_address[df$street_address == "Yarra Building Federation Square"] <- "Federation Square"
df$street_address[df$street_address == "Zoological Gardens Elliott Avenue"] <- "Elliott Avenue"

# Remove street_type 
df <- df %>% select(-any_of("street_type"))

# Save
write_csv(df, "data/df_with_streets.csv")

# Also save updated unique streets
unique_streets <- df %>%
  filter(!is.na(street_address)) %>%
  distinct(street_address) %>%
  arrange(street_address)

write_csv(unique_streets, "data/unique_streets.csv")




```






```{r}
library(tidyverse)

# Load data
df <- read_csv("data/df_with_streets.csv")



# Get one row per business
business_level <- df %>%
  group_by(business_id) %>%
  slice_head(n = 1) %>%
  ungroup()


# Calculator all streets


street_stats_all <- business_level %>%
  filter(
    !is.na(street_address),      # Must have street
    bias_filter == FALSE          # Bias-corrected sample
  ) %>%
  group_by(street_address) %>%
  summarise(
    total_businesses = n(),
    closed_businesses = sum(!str_detect(business_status, "Operating")),
    operating_12plus = sum(str_detect(business_status, "Long-term Success")),
    
    # calculated for ALL streets regardless of sample size
    street_avg_lifespan = mean(lifespan_years, na.rm = TRUE),
    street_median_lifespan = median(lifespan_years, na.rm = TRUE),
    street_success_rate = mean(lifespan_group == ">12 yrs") * 100,
    
    street_avg_seats = mean(num_seats, na.rm = TRUE),
    street_median_seats = median(num_seats, na.rm = TRUE),
    
    # Add reliability indicator based on sample size
    reliability = case_when(
      total_businesses >= 10 ~ "High (10+)",
      total_businesses >= 5 ~ "Medium (5-9)",
      total_businesses >= 3 ~ "Low (3-4)",
      TRUE ~ "Very Low (1-2)"
    ),
    
    .groups = "drop"
  ) %>%
  arrange(desc(street_success_rate))

cat("Total streets with calculated stats:", nrow(street_stats_all), "\n\n")

# Show distribution by reliability
cat("STREET DISTRIBUTION BY SAMPLE SIZE:\n")
reliability_dist <- street_stats_all %>%
  count(reliability) %>%
  mutate(pct = round(100 * n / sum(n), 1))
print(reliability_dist)


# Join to full dadaset


df_final <- df %>%
  left_join(
    street_stats_all %>% 
      select(street_address, street_avg_lifespan, street_success_rate, 
             street_median_seats, total_businesses, reliability),
    by = "street_address"
  ) %>%
  mutate(
    street_stats_imputed = is.na(street_address)
  )

# Check coverage
coverage <- df_final %>%
  summarise(
    total_records = n(),
    has_calculated_stats = sum(!is.na(street_avg_lifespan)),
    needs_imputation = sum(is.na(street_avg_lifespan))
  )



if (coverage$needs_imputation > 0) {
  cat("\nImputing for records with no extractable street...\n")
  
  overall_medians <- df_final %>%
    filter(!is.na(street_avg_lifespan)) %>%
    summarise(
      median_lifespan = median(street_avg_lifespan),
      median_success = median(street_success_rate),
      median_seats = median(street_median_seats)
    )
  
  df_final <- df_final %>%
    mutate(
      street_avg_lifespan = coalesce(street_avg_lifespan, overall_medians$median_lifespan),
      street_success_rate = coalesce(street_success_rate, overall_medians$median_success),
      street_median_seats = coalesce(street_median_seats, overall_medians$median_seats),
      total_businesses = coalesce(total_businesses, 0L),
      reliability = coalesce(reliability, "Imputed")
    )
}



# SAVE


write_csv(df_final, "df_with_street_stats_ALL_STREETS.csv")
write_csv(street_stats_all, "street_statistics_ALL_317.csv")


```






```{r}
library(tidyverse)
library(sf)


# 1. Load data


df_source <- read_csv("df_with_street_stats_ALL_STREETS.csv")


# 2. Calculate spatial competition (300m)




# Get business entry points (ALL businesses for true market density)
business_entry_all <- df_source %>%
  group_by(business_id) %>%
  arrange(census_year) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  select(business_id, entry_year = census_year, industry, lat, lon, num_seats)

# Convert to spatial
business_sf <- st_as_sf(
  business_entry_all %>% filter(!is.na(lat), !is.na(lon)),
  coords = c("lon", "lat"),
  crs = 4326
) %>% st_transform(28355)

radius_m <- 300
n <- nrow(business_sf)

competition_metrics <- data.frame(
  business_id = business_sf$business_id,
  entry_year = business_sf$entry_year,
  industry = business_sf$industry,
  competitors_300m = numeric(n),
  same_industry_300m = numeric(n),
  other_industry_300m = numeric(n)
)

buffers <- st_buffer(business_sf, dist = radius_m)

cat("Processing", n, "businesses...\n")
pb <- txtProgressBar(max = n, style = 3)

for (i in 1:n) {
  if (i %% 500 == 0) setTxtProgressBar(pb, i)
  
  focal_year <- business_sf$entry_year[i]
  focal_industry <- business_sf$industry[i]
  
  # Businesses existing at entry year
  existing <- business_sf$entry_year <= focal_year
  
  # Total competition
  overlaps <- st_intersects(buffers[i,], business_sf[existing,], sparse = FALSE)
  competition_metrics$competitors_300m[i] <- sum(overlaps) - 1
  
  # Same industry
  same_ind <- existing & business_sf$industry == focal_industry
  overlaps_same <- st_intersects(buffers[i,], business_sf[same_ind,], sparse = FALSE)
  competition_metrics$same_industry_300m[i] <- sum(overlaps_same) - 1
  
  # Other industry
  other_ind <- existing & business_sf$industry != focal_industry
  overlaps_other <- st_intersects(buffers[i,], business_sf[other_ind,], sparse = FALSE)
  competition_metrics$other_industry_300m[i] <- sum(overlaps_other) - 1
}

close(pb)

# Add derived metrics
competition_metrics <- competition_metrics %>%
  mutate(
    high_competition = as.numeric(competitors_300m >= 15),
    industry_ratio = same_industry_300m / (competitors_300m + 1),
    competition_cat = case_when(
      competitors_300m == 0 ~ "Isolated",
      competitors_300m <= 5 ~ "Very Low (1-5)",
      competitors_300m <= 15 ~ "Low (6-15)",
      competitors_300m <= 30 ~ "Medium (16-30)",
      competitors_300m <= 50 ~ "High (31-50)",
      TRUE ~ "Very High (>50)"
    )
  )




competition_metrics %>% 
  count(competition_cat) %>% 
  mutate(pct = round(100*n/sum(n), 1)) %>%
  print()


competition_metrics %>%
  group_by(industry) %>%
  summarise(
    n = n(),
    mean = round(mean(competitors_300m), 1),
    median = median(competitors_300m),
    p75 = quantile(competitors_300m, 0.75),
    max = max(competitors_300m),
    .groups = "drop"
  ) %>% print()


# Prepare modelling samples



# One row per business
business_level <- df_source %>%
  group_by(business_id) %>%
  arrange(census_year) %>%
  slice_head(n = 1) %>%
  ungroup()



# Apply bias filter
df_model <- business_level %>%
  filter(bias_filter == FALSE)


# Join competetion metrics




df_with_comp <- df_model %>%
  left_join(
    competition_metrics %>% 
      select(business_id, competitors_300m, same_industry_300m, 
             other_industry_300m, high_competition, industry_ratio, competition_cat),
    by = "business_id"
  )


# 5. Select streets based on performance




street_performance <- df_with_comp %>%
  filter(!is.na(street_address)) %>%
  group_by(street_address) %>%
  summarise(
    n_businesses = n(),
    avg_lifespan = mean(lifespan_years, na.rm = TRUE),
    success_rate = mean(lifespan_group == ">12 yrs") * 100,
    median_lifespan = median(lifespan_years, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(n_businesses >= 5)


# Top 20 by success rate
top20_success <- street_performance %>%
  arrange(desc(success_rate)) %>%
  head(20)



# Bottom 20 by success rate
bottom20_success <- street_performance %>%
  arrange(success_rate) %>%
  head(20)



# Top 20 by avg lifespan
top20_lifespan <- street_performance %>%
  arrange(desc(avg_lifespan)) %>%
  head(20)



# Bottom 20 by avg lifespan
bottom20_lifespan <- street_performance %>%
  arrange(avg_lifespan) %>%
  head(20)



# Combine
best_streets <- unique(c(top20_success$street_address, top20_lifespan$street_address))
worst_streets <- unique(c(bottom20_success$street_address, bottom20_lifespan$street_address))
selected_streets <- unique(c(best_streets, worst_streets))


# feature processing



df_features <- df_with_comp %>%
  mutate(
    # Target variable
    lifespan_numeric = lifespan_years,
    
    # Business characteristics
    industry_cafe = as.numeric(industry == "Cafes"),
    log_seats = log1p(num_seats),
    
    # Size categories
    size_small = as.numeric(num_seats <= 40),
    size_medium = as.numeric(num_seats > 40 & num_seats <= 80),
    size_large = as.numeric(num_seats > 80),
    
    # Competition metrics
    comp_300m = competitors_300m,
    same_ind_300m = same_industry_300m,
    other_ind_300m = other_industry_300m,
    comp_density = competitors_300m / 100,
    industry_dominance = same_industry_300m / (competitors_300m + 1),
    
    # Competition flags
    high_comp = as.numeric(competitors_300m >= 30),
    very_high_comp = as.numeric(competitors_300m >= 50),
    isolated = as.numeric(competitors_300m == 0),
    
    # Temporal
    entry_year_norm = (census_year - 2002) / (2023 - 2002),
    is_recent = as.numeric(census_year >= 2015),
    
    # Location
    dist_cbd = sqrt((lon - 144.9631)^2 + (lat - (-37.8136))^2) * 111,
    
    # Property
    property_crowding = as.numeric(businesses_at_property >= 3)
  )

# Create street dummies for selected streets
for (street in selected_streets) {
  col_name <- paste0("street_", str_replace_all(str_to_lower(street), "[^a-z0-9]", "_"))
  df_features[[col_name]] <- as.numeric(df_features$street_address == street)
}

# Add street category indicators
df_features <- df_features %>%
  mutate(
    on_best_street = as.numeric(street_address %in% best_streets),
    on_worst_street = as.numeric(street_address %in% worst_streets)
  )


# Validation


# Check for duplicate columns
dup_cols <- names(df_features)[duplicated(names(df_features))]
if (length(dup_cols) > 0) {
  cat("WARNING: Duplicate columns found:", paste(dup_cols, collapse = ", "), "\n")
} else {
  cat("✓ No duplicate columns\n")
}

# Check NAs in key features
key_features <- c("lifespan_numeric", "industry_cafe", "log_seats", 
                  "comp_300m", "same_ind_300m", "entry_year_norm", "dist_cbd")

na_summary <- df_features %>%
  summarise(across(all_of(key_features), ~sum(is.na(.))))

cat("\nNA counts in key features:\n")
print(t(na_summary))

# Summary by street type
cat("\n\nOutcomes by street type:\n")
df_features %>%
  mutate(street_type = case_when(
    on_best_street == 1 ~ "Best Streets",
    on_worst_street == 1 ~ "Worst Streets",
    TRUE ~ "Other Streets"
  )) %>%
  group_by(street_type) %>%
  summarise(
    n = n(),
    mean_lifespan = round(mean(lifespan_numeric, na.rm = TRUE), 1),
    success_rate = round(mean(lifespan_group == ">12 yrs") * 100, 1),
    .groups = "drop"
  ) %>% print()

# Competition summary in modeling sample
cat("\n\nCompetition in modeling sample:\n")
df_features %>%
  summarise(
    n = n(),
    mean_comp = round(mean(comp_300m, na.rm = TRUE), 1),
    median_comp = median(comp_300m, na.rm = TRUE),
    q75 = quantile(comp_300m, 0.75, na.rm = TRUE),
    max_comp = max(comp_300m, na.rm = TRUE)
  ) %>% print()


# 8. SAVE OUTPUTS

write_csv(df_features, "df_modeling_final.csv")
write_csv(competition_metrics, "competition_300m_all.csv")
write_csv(street_performance, "street_performance_all.csv")

# Create street selection summary
street_summary <- data.frame(
  street_address = selected_streets,
  category = case_when(
    selected_streets %in% best_streets & selected_streets %in% worst_streets ~ "Both",
    selected_streets %in% best_streets ~ "Best",
    TRUE ~ "Worst"
  )
) %>%
  left_join(street_performance, by = "street_address") %>%
  arrange(desc(success_rate))

write_csv(street_summary, "selected_streets_summary.csv")


```

#### Prepare model data

```{r}
library(tidyverse)
library(sf)



# Load source data
df_source <- read_csv("df_with_street_stats_ALL_STREETS.csv")

# Prepare spatial data
business_entry <- df_source %>%
  group_by(business_id) %>%
  arrange(census_year) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  select(business_id, entry_year = census_year, industry, lat, lon) %>%
  filter(!is.na(lat), !is.na(lon))

valid_business_ids <- business_entry$business_id

business_sf <- st_as_sf(
  business_entry,
  coords = c("lon", "lat"),
  crs = 4326
) %>% st_transform(28355)

# Calculate competition at 200m
calculate_competition <- function(business_sf, radius_m) {
  
  n <- nrow(business_sf)
  
  competition <- data.frame(
    business_id = business_sf$business_id,
    entry_year = business_sf$entry_year,
    industry = business_sf$industry,
    comp_total = numeric(n),
    comp_same = numeric(n),
    comp_other = numeric(n)
  )
  
  buffers <- st_buffer(business_sf, dist = radius_m)
  pb <- txtProgressBar(max = n, style = 3)
  
  for (i in 1:n) {
    if (i %% 500 == 0) setTxtProgressBar(pb, i)
    
    focal_year <- business_sf$entry_year[i]
    focal_industry <- business_sf$industry[i]
    existing_mask <- business_sf$entry_year <= focal_year
    
    # Total competition
    overlaps_total <- st_intersects(buffers[i,], business_sf[existing_mask,], sparse = FALSE)
    competition$comp_total[i] <- max(0, sum(overlaps_total) - 1)
    
    # Same industry
    same_mask <- existing_mask & business_sf$industry == focal_industry
    overlaps_same <- st_intersects(buffers[i,], business_sf[same_mask,], sparse = FALSE)
    competition$comp_same[i] <- max(0, sum(overlaps_same) - 1)
    
    # Other industry
    other_mask <- existing_mask & business_sf$industry != focal_industry
    overlaps_other <- st_intersects(buffers[i,], business_sf[other_mask,], sparse = FALSE)
    competition$comp_other[i] <- max(0, sum(overlaps_other))
  }
  
  close(pb)
  return(competition)
}

# Calculate 200m competition
comp_200m <- calculate_competition(business_sf, 200)

# Prepare base modeling data
business_level <- df_source %>%
  group_by(business_id) %>%
  arrange(census_year) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  filter(bias_filter == FALSE) %>%
  filter(business_id %in% valid_business_ids)

# Create CLUE area dummies
clue_counts <- business_level %>%
  filter(!is.na(clue_area)) %>%
  count(clue_area, sort = TRUE)

top_clue <- head(clue_counts$clue_area, 10)

# Create street quality indicators
street_perf <- business_level %>%
  filter(!is.na(street_address)) %>%
  group_by(street_address) %>%
  summarise(
    n_businesses = n(),
    avg_lifespan = mean(lifespan_years, na.rm = TRUE),
    success_rate = mean(lifespan_group == ">12 yrs") * 100,
    .groups = "drop"
  ) %>%
  filter(n_businesses >= 5)

success_p80 <- quantile(street_perf$success_rate, 0.80)
success_p20 <- quantile(street_perf$success_rate, 0.20)
lifespan_p80 <- quantile(street_perf$avg_lifespan, 0.80)
lifespan_p20 <- quantile(street_perf$avg_lifespan, 0.20)

best_streets <- street_perf %>%
  filter(success_rate >= success_p80 & avg_lifespan >= lifespan_p80) %>%
  pull(street_address)

worst_streets <- street_perf %>%
  filter(success_rate <= success_p20 & avg_lifespan <= lifespan_p20) %>%
  pull(street_address)

# Build final dataset
df_model <- business_level %>%
  inner_join(comp_200m %>% select(business_id, comp_total, comp_same, comp_other), 
             by = "business_id")

df_features <- df_model %>%
  mutate(
    lifespan_numeric = lifespan_years,
    industry_cafe = as.numeric(industry == "Cafes"),
    log_seats = log1p(num_seats),
    outdoor_seating = as.numeric(str_detect(seating_type, "Outdoor")),
    mixed_seating = as.numeric(str_detect(seating_type, "Mixed")),
    same_ind = comp_same,
    other_ind = comp_other,
    on_best_street = as.numeric(street_address %in% best_streets),
    on_worst_street = as.numeric(street_address %in% worst_streets),
    property_competition = businesses_at_property - 1
  )

# Add CLUE area dummies
for (area in top_clue) {
  col_name <- paste0("clue_", str_replace_all(str_to_lower(area), "[^a-z0-9]", "_"))
  df_features[[col_name]] <- as.numeric(df_features$clue_area == area)
}

# Select final features
final_features <- c(
  "lifespan_numeric",
  "industry_cafe", "log_seats", "outdoor_seating", "mixed_seating",
  "same_ind", "other_ind",
  "on_best_street", "on_worst_street",
  paste0("clue_", str_replace_all(str_to_lower(top_clue), "[^a-z0-9]", "_")),
  "property_competition"
)

df_final <- df_features %>%
  select(all_of(final_features))

# Save
write_csv(df_final, "df_clean_200m.csv")

# Summary
summary_stats <- tibble(
  metric = c("Total rows", "Features", "Mean same_ind", "Mean other_ind", "NAs present"),
  value = c(
    nrow(df_final),
    ncol(df_final),
    round(mean(df_final$same_ind), 1),
    round(mean(df_final$other_ind), 1),
    sum(is.na(df_final))
  )
)

print(summary_stats)


```







```{r}
library(tidyverse)
library(randomForest)
library(ggplot2)
library(knitr)



# Load data
df_best <- read_csv("df_clean_200m.csv", show_col_types = FALSE)

# Train model
set.seed(42)
best_rf <- randomForest(
  lifespan_numeric ~ .,
  data = df_best,
  ntree = 500,
  importance = TRUE,
  nodesize = 20
)

# Extract metrics
y_actual <- df_best$lifespan_numeric
y_pred <- predict(best_rf, df_best)
residuals <- y_actual - y_pred

oob_r2 <- tail(best_rf$rsq, 1)
oob_rmse <- sqrt(tail(best_rf$mse, 1))
insample_r2 <- cor(y_actual, y_pred)^2
insample_mae <- mean(abs(residuals))
baseline_rmse <- sqrt(mean((y_actual - mean(y_actual))^2))



table1_performance <- tibble(
  Metric = c(
    "OOB R² (Out-of-Bag)",
    "OOB RMSE",
    "Mean Absolute Error (MAE)",
    "Baseline RMSE (Always Predict Mean)",
    "Model Improvement vs Baseline"
  ),
  Value = c(
    sprintf("%.4f", oob_r2),
    sprintf("%.2f", oob_rmse),
    sprintf("%.2f", insample_mae),
    sprintf("%.2f", baseline_rmse),
    sprintf("%.1f%%", (baseline_rmse - oob_rmse) / baseline_rmse * 100)
  ),
)



importance_raw <- data.frame(
  feature = rownames(importance(best_rf)),
  inc_mse = importance(best_rf)[, "%IncMSE"],
  inc_purity = importance(best_rf)[, "IncNodePurity"]
)

correlations <- df_best %>%
  select(-lifespan_numeric) %>%
  summarise(across(everything(), ~cor(., df_best$lifespan_numeric))) %>%
  pivot_longer(everything(), names_to = "feature", values_to = "correlation")

table2_importance <- importance_raw %>%
  left_join(correlations, by = "feature") %>%
  arrange(desc(inc_mse)) %>%
  mutate(
    rank = row_number(),
    feature_clean = case_when(
      feature == "same_ind" ~ "Same-Industry Competition (200m)",
      feature == "other_ind" ~ "Other-Industry Competition (200m)",
      feature == "log_seats" ~ "Restaurant Size (log seats)",
      feature == "on_best_street" ~ "Prime Location Street",
      feature == "on_worst_street" ~ "Poor Quality Street",
      feature == "property_competition" ~ "Property-Level Competition",
      feature == "industry_cafe" ~ "Cafe Business Model",
      feature == "outdoor_seating" ~ "Outdoor Seating",
      feature == "mixed_seating" ~ "Mixed Seating Type",
      feature == "clue_melbourne__cbd_" ~ "Melbourne CBD Location",
      feature == "clue_docklands" ~ "Docklands Area",
      feature == "clue_carlton" ~ "Carlton Area",
      feature == "clue_southbank" ~ "Southbank Area",
      TRUE ~ str_to_title(str_replace_all(feature, "_", " "))
    ),
    effect_direction = case_when(
      correlation < -0.02 ~ "Negative ↓",
      correlation > 0.02 ~ "Positive ↑",
      TRUE ~ "Neutral"
    )
  ) %>%
  head(10) %>%
  select(
    Rank = rank,
    Feature = feature_clean,
    Importance = inc_mse,
    `Effect on Lifespan` = effect_direction,
    Correlation = correlation
  ) %>%
  mutate(
    Importance = round(Importance, 1),
    Correlation = sprintf("%+.3f", Correlation)
  )



table3_negative <- importance_raw %>%
  left_join(correlations, by = "feature") %>%
  filter(correlation < -0.02) %>%
  arrange(desc(inc_mse)) %>%
  mutate(
    feature_clean = case_when(
      feature == "same_ind" ~ "Same-Industry Competition",
      feature == "other_ind" ~ "Other-Industry Competition",
      feature == "property_competition" ~ "Property-Level Competition",
      feature == "on_worst_street" ~ "Poor Quality Street",
      feature == "clue_docklands" ~ "Docklands Location",
      TRUE ~ str_to_title(str_replace_all(feature, "_", " "))
    ),
    effect_size = case_when(
      abs(correlation) > 0.15 ~ "Strong",
      abs(correlation) > 0.08 ~ "Moderate",
      TRUE ~ "Weak"
    )
  ) %>%
  head(8) %>%
  select(
    Feature = feature_clean,
    Importance = inc_mse,
    `Correlation with Lifespan` = correlation,
    `Effect Strength` = effect_size
  ) %>%
  mutate(
    Importance = round(Importance, 1),
    `Correlation with Lifespan` = sprintf("%+.3f", `Correlation with Lifespan`)
  )



table4_positive <- importance_raw %>%
  left_join(correlations, by = "feature") %>%
  filter(correlation > 0.02) %>%
  arrange(desc(inc_mse)) %>%
  mutate(
    feature_clean = case_when(
      feature == "log_seats" ~ "Restaurant Size (Larger)",
      feature == "on_best_street" ~ "Prime Location Street",
      feature == "clue_melbourne__cbd_" ~ "Melbourne CBD Location",
      feature == "industry_cafe" ~ "Cafe Business Model",
      feature == "clue_carlton" ~ "Carlton Location",
      feature == "outdoor_seating" ~ "Outdoor Seating",
      TRUE ~ str_to_title(str_replace_all(feature, "_", " "))
    ),
    effect_size = case_when(
      abs(correlation) > 0.15 ~ "Strong",
      abs(correlation) > 0.08 ~ "Moderate",
      TRUE ~ "Weak"
    )
  ) %>%
  head(8) %>%
  select(
    Feature = feature_clean,
    Importance = inc_mse,
    `Correlation with Lifespan` = correlation,
    `Effect Strength` = effect_size
  ) %>%
  mutate(
    Importance = round(Importance, 1),
    `Correlation with Lifespan` = sprintf("%+.3f", `Correlation with Lifespan`)
  )



same_ind_stats <- df_best %>%
  summarise(
    mean = mean(same_ind),
    median = median(same_ind),
    q25 = quantile(same_ind, 0.25),
    q75 = quantile(same_ind, 0.75),
    q90 = quantile(same_ind, 0.90)
  )

other_ind_stats <- df_best %>%
  summarise(
    mean = mean(other_ind),
    median = median(other_ind),
    q25 = quantile(other_ind, 0.25),
    q75 = quantile(other_ind, 0.75),
    q90 = quantile(other_ind, 0.90)
  )

table5_competition <- tibble(
  `Competition Type` = c("Same-Industry (Restaurants/Cafes)", "Other-Industry (Bars/Pubs/Fast Food)"),
  Mean = c(round(same_ind_stats$mean, 1), round(other_ind_stats$mean, 1)),
  Median = c(same_ind_stats$median, other_ind_stats$median),
  `25th %ile` = c(same_ind_stats$q25, other_ind_stats$q25),
  `75th %ile` = c(same_ind_stats$q75, other_ind_stats$q75),
  `90th %ile (High Risk)` = c(same_ind_stats$q90, other_ind_stats$q90),
  `Effect on Survival` = c(
    "Each competitor reduces lifespan",
    "Strongest negative predictor"
  )
)



table6_reliability <- tibble(
  `Reliability Check` = c(
    "Sample Size",
    "Feature Count",
    "Predictions within ±3 years",
    "Predictions within ±5 years",
    "Mean Residual (Bias Check)",
    "Model Convergence",
    "Overfit Gap (In-sample - OOB R²)"
  ),
  Result = c(
    sprintf("%d businesses", nrow(df_best)),
    sprintf("%d features", ncol(df_best) - 1),
    sprintf("%.1f%%", mean(abs(residuals) <= 3) * 100),
    sprintf("%.1f%%", mean(abs(residuals) <= 5) * 100),
    sprintf("%.3f (near zero = unbiased)", mean(residuals)),
    "Converged at ~400 trees",
    sprintf("%.1f pp (acceptable)", (insample_r2 - oob_r2) * 100)
  ),
  Assessment = c(
    "✓ Adequate",
    "✓ Reasonable",
    ifelse(mean(abs(residuals) <= 3) >= 0.5, "✓ Good", "⚠ Fair"),
    ifelse(mean(abs(residuals) <= 5) >= 0.7, "✓ Good", "⚠ Fair"),
    ifelse(abs(mean(residuals)) < 0.1, "✓ Unbiased", "⚠ Slight bias"),
    "✓ Stable",
    ifelse((insample_r2 - oob_r2) < 0.05, "✓ Low overfit", "⚠ Moderate overfit")
  )
)






# Save visualizations 
invisible({
  
  # Plot 1: Top 15 Features
  p1 <- table2_importance %>%
    bind_rows(
      importance_raw %>%
        left_join(correlations, by = "feature") %>%
        arrange(desc(inc_mse)) %>%
        slice(11:15) %>%
        mutate(
          feature_clean = str_to_title(str_replace_all(feature, "_", " ")),
          effect_direction = ifelse(correlation > 0, "Positive ↑", "Negative ↓"),
          Rank = 11:15,
          Importance = round(inc_mse, 1),
          Correlation = sprintf("%+.3f", correlation)
        ) %>%
        select(Rank, Feature = feature_clean, Importance, 
               `Effect on Lifespan` = effect_direction, Correlation)
    ) %>%
    mutate(
      Feature = reorder(Feature, Importance),
      effect_color = ifelse(`Effect on Lifespan` == "Positive ↑", "Positive", "Negative")
    ) %>%
    ggplot(aes(x = Importance, y = Feature, fill = effect_color)) +
    geom_col() +
    scale_fill_manual(values = c("Positive" = "#10b981", "Negative" = "#dc2626")) +
    labs(title = "Feature Importance: Top 15 Predictors",
         subtitle = "Green = increases lifespan | Red = decreases lifespan",
         x = "Importance (% Increase in MSE)",
         y = NULL,
         fill = "Effect") +
    theme_minimal(base_size = 11) +
    theme(plot.title = element_text(face = "bold"),
          legend.position = "bottom")
  
  ggsave("model_importance_top15.png", p1, width = 12, height = 8, dpi = 300)
  
  # Plot 2: Predictions vs Actual
  p2 <- data.frame(actual = y_actual, predicted = y_pred) %>%
    ggplot(aes(x = actual, y = predicted)) +
    geom_point(alpha = 0.3, color = "#3b82f6") +
    geom_abline(slope = 1, intercept = 0, color = "#dc2626", 
                linetype = "dashed", size = 1) +
    annotate("text", x = 2, y = 22,
             label = sprintf("OOB R² = %.3f\nRMSE = %.2f years", oob_r2, oob_rmse),
             hjust = 0, size = 5) +
    labs(title = "Model Predictions vs Actual Lifespan",
         subtitle = "Dashed line = perfect prediction",
         x = "Actual Lifespan (years)",
         y = "Predicted Lifespan (years)") +
    theme_minimal(base_size = 11) +
    theme(plot.title = element_text(face = "bold"))
  
  ggsave("model_predictions_vs_actual.png", p2, width = 9, height = 9, dpi = 300)
})


list(
  performance = table1_performance,
  importance_top10 = table2_importance,
  negative_factors = table3_negative,
  positive_factors = table4_positive,
  competition_summary = table5_competition,
  reliability = table6_reliability
)

```























































































